{"cells":[{"cell_type":"markdown","source":["The codes below can be used for reproducing Fig 1 and Fig 9"],"metadata":{"id":"2gMY5PvKHJWZ"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"elapsed":4455,"status":"error","timestamp":1759805924660,"user":{"displayName":"Hossein Taheri","userId":"10565663845480396106"},"user_tz":420},"id":"BR4C0YIkCQOX","outputId":"e022f02d-d379-4228-a3e3-49c6727f46ab"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4146017885.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math as m\n","# Define the quadratic activation function\n","import random\n","\n","seed = 1  # fixed seed\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","class QuadraticActivation(nn.Module):\n","    def forward(self, x):\n","        return (x ** 2)/2\n","\n","# Linear loss defined for +/-1 labels\n","class logloss(nn.Module):\n","    def __init__(self):\n","        super(logloss, self).__init__()\n","\n","    def forward(self, preds, labels):\n","        # Example custom loss: Mean Absolute Error\n","        loss = torch.mean(-preds*labels)\n","        return loss\n","# Define the neural network model\n","class OneHiddenLayerNN(nn.Module):\n","    def __init__(self):\n","        super(OneHiddenLayerNN, self).__init__()\n","        self.fc1 = nn.Linear(dimension, num_neurons)\n","        self.fc2 = nn.Linear(num_neurons, 1, bias=False)\n","        self.fc2.weight.data = torch.from_numpy(np.random.choice([-1/num_neurons**0.5, 1/num_neurons**0.5], size=(1, num_neurons))).float() # second layer weights, fixed\n","        torch.nn.init.normal_(self.fc1.weight,\n","                        mean=0, std=1) # first layer initialization\n","        self.activation = QuadraticActivation()\n","        #nn.init.zeros_(self.fc1.weight)\n","    def forward(self, x):\n","        x = self.activation(self.fc1(x)) # normalized by the number of neuronsß\n","        x = self.fc2(x)\n","        return x\n","\n","#ax.set_facecolor('ghostwhite')\n","\n","  # num_points = 100000 # total number of points in the distribution\n","  # data_points = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, d), replace=True)).float()\n","  # # Generate the labels\n","  # labels = torch.sign(data_points[:, 0] * data_points[:, 1])\n","  # #\n","\n","  # labels = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, 1))).float()\n","\n","\n","\n","# Parameters\n","num_data_points = 10000  # Total number of data points\n","num_train_points = 4000 # training-set size\n","\n","dimension = 50          # Dimension of each data point\n","\n","num_samples_per_class = num_data_points // 2  # Data points per class\n","\n","# Mean vectors for the two classes, generated as tensors\n","#mu_1 = torch.randn(dimension)/dimension  # Mean vector for class 1\n","#mu_2 = torch.randn(dimension)/dimension # Mean vector for class -1\n","mu_1 = torch.zeros(dimension)\n","mu_2= torch.zeros(dimension)\n","mu_1[:2]=1/dimension**0.5\n","mu_2[0]=1/dimension**0.5\n","mu_2[1]=-1/dimension**0.5\n","stdd = 0.1/dimension**0.5\n","# Generate data points for class 1 (label +1)\n","data_class_1_pos = torch.normal(mean=mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_1_neg = torch.normal(mean=-mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_1 = torch.cat((data_class_1_pos, data_class_1_neg), dim=0)\n","labels_class_1 = torch.ones(num_samples_per_class)\n","\n","# Generate data points for class -1 (label -1)\n","data_class_2_pos = torch.normal(mean=mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_2_neg = torch.normal(mean=-mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_2 = torch.cat((data_class_2_pos, data_class_2_neg), dim=0)\n","labels_class_2 = -torch.ones(num_samples_per_class)\n","\n","# Combine the data and labels\n","data = torch.cat((data_class_1, data_class_2), dim=0)\n","labels = torch.cat((labels_class_1, labels_class_2), dim=0)\n","\n","\n","\n","# Split the data into training and test sets\n","train_indices = torch.randperm(num_data_points)[:num_train_points]\n","test_indices = torch.randperm(num_data_points)[num_train_points:]\n","\n","train_data = data[train_indices]\n","train_labels = labels[train_indices]\n","test_data = data[test_indices]\n","test_labels = labels[test_indices]\n","\n","\n","\n","\n","\n","# Parameters\n","\n","# Mean vectors for the two classes, generated as tensors\n","#mu_11 = torch.randn(dimension)/dimension  # Mean vector for class 1\n","#mu_22 = torch.randn(dimension)/dimension  # Mean vector for class -1\n","mu_11 = torch.zeros(dimension)\n","mu_22= torch.zeros(dimension)\n","mu_11[2:4]=1/dimension**0.5\n","mu_22[2]=1/dimension**0.5\n","mu_22[3]=-1/dimension**0.5\n","# Generate data points for class 1 (label +1)\n","data_class_11_pos = torch.normal(mean=mu_11.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_11_neg = torch.normal(mean=-mu_11.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_11 = torch.cat((data_class_11_pos, data_class_11_neg), dim=0)\n","labels_class_11 = torch.ones(num_samples_per_class)\n","\n","# Generate data points for class -1 (label -1)\n","data_class_22_pos = torch.normal(mean=mu_22.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_22_neg = torch.normal(mean=-mu_22.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_22 = torch.cat((data_class_22_pos, data_class_22_neg), dim=0)\n","labels_class_22 = -torch.ones(num_samples_per_class)\n","\n","# Combine the data and labels\n","data2 = torch.cat((data_class_11, data_class_22), dim=0)\n","labels2 = torch.cat((labels_class_11, labels_class_22), dim=0)\n","\n","\n","\n","# Split the data into training and test sets\n","train_indices2 = torch.randperm(num_data_points)[:num_train_points]\n","test_indices2 = torch.randperm(num_data_points)[num_train_points:]\n","\n","train_data2 = data2[train_indices2]\n","train_labels2 = labels2[train_indices2]\n","test_data2 = data2[test_indices2]\n","test_labels2 = labels2[test_indices2]\n","\n","\n","\n","\n","\n","mu_111 = torch.zeros(dimension)\n","mu_222= torch.zeros(dimension)\n","mu_111[4:6]=1/dimension**0.5\n","mu_222[4]=1/dimension**0.5\n","mu_222[5]=-1/dimension**0.5\n","# Generate data points for class 1 (label +1)\n","data_class_111_pos = torch.normal(mean=mu_111.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_111_neg = torch.normal(mean=-mu_111.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_111 = torch.cat((data_class_111_pos, data_class_111_neg), dim=0)\n","labels_class_111 = torch.ones(num_samples_per_class)\n","\n","# Generate data points for class -1 (label -1)\n","data_class_222_pos = torch.normal(mean=mu_222.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_222_neg = torch.normal(mean=-mu_222.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_222 = torch.cat((data_class_222_pos, data_class_222_neg), dim=0)\n","labels_class_222 = -torch.ones(num_samples_per_class)\n","\n","# Combine the data and labels\n","data3 = torch.cat((data_class_111, data_class_222), dim=0)\n","labels3 = torch.cat((labels_class_111, labels_class_222), dim=0)\n","\n","\n","\n","# Split the data into training and test sets\n","train_indices3 = torch.randperm(num_data_points)[:num_train_points]\n","test_indices3 = torch.randperm(num_data_points)[num_train_points:]\n","\n","train_data3 = data3[train_indices3]\n","train_labels3 = labels3[train_indices3]\n","test_data3 = data3[test_indices3]\n","test_labels3 = labels3[test_indices3]\n","\n","\n","\n","\n","num_neurons = 1000\n","learning_rate = 2 # stepsize\n","\n","\n","\n","# Initialize the model, loss function, and optimizer\n","model = OneHiddenLayerNN()\n","criterion = logloss()\n","optimizer = torch.optim.SGD(model.fc1.parameters(), lr=learning_rate)  # Fix the weights of the second layer\n","\n","# Lists to store training and test errors\n","test_errors = []\n","test_errors2 = []\n","test_errors3 = []\n","num_iterations = round(200)\n","# Training loop\n","mem = 0\n","k= 0\n","tensor0 = model.fc1.weight.clone()\n","weight_norm = []\n","for i in range(3*num_iterations):\n","\n","    with torch.no_grad():\n","      # Calculate test error\n","        test_outputs = model(test_data)\n","        test_predicted_labels = torch.sign(test_outputs.squeeze())\n","        test_error = torch.mean((test_predicted_labels != test_labels).float())\n","        test_errors.append(test_error.item())\n","\n","        test_outputs2 = model(test_data2)\n","        test_predicted_labels2 = torch.sign(test_outputs2.squeeze())\n","        test_error2 = torch.mean((test_predicted_labels2 != test_labels2).float())\n","        test_errors2.append(test_error2.item())\n","\n","        test_outputs3 = model(test_data3)\n","        test_predicted_labels3 = torch.sign(test_outputs3.squeeze())\n","        test_error3 = torch.mean((test_predicted_labels3 != test_labels3).float())\n","        test_errors3.append(test_error3.item())\n","\n","        tensor = [i for i in model.fc1.weight]\n","        weight_norm.append(torch.norm(tensor[1]-tensor0[1]))\n","        # Forward pass\n","    if i<num_iterations:\n","        outputs = model(train_data)\n","        loss = criterion(outputs.squeeze(), train_labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    elif i<2*num_iterations:\n","        outputs2 = model(train_data2)\n","        loss = criterion(outputs2.squeeze(), train_labels2)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    else:\n","        outputs3 = model(train_data3)\n","        loss = criterion(outputs3.squeeze(), train_labels3)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","      # Backward pass and optimization\n","\n","    if i%100==0:\n","        print(\"iteration =\",i)\n","        #print(mem)\n","\n","\n","fig4 = plt.figure(\"Figure 1\")\n","fig4, ax = plt.subplots()\n","plt.plot(range(3*num_iterations), test_errors,linewidth =2,color=\"green\")\n","\n","plt.plot(range(3*num_iterations), [np.nan]*num_iterations+test_errors2[num_iterations:],linewidth =2,color=\"red\")\n","plt.plot(range(3*num_iterations), [np.nan]*2*num_iterations+test_errors3[2*num_iterations:],linewidth =2,color=\"blue\")\n","\n","plt.plot([num_iterations]*2,[0,0.8],'--',linewidth =1,color=\"black\")\n","plt.plot([2*num_iterations]*2,[0,0.8],'--',linewidth =1,color=\"black\")\n","ax.set_xlim(left=0)\n","ax.set_ylim(top=0.5)\n","ax.set_ylim(bottom=-.01)\n","ax.text(80, 0.45, \"Task 1\", fontsize=8, color='green')\n","ax.text(280, 0.45, \"Task 2\", fontsize=8, color='red')\n","ax.text(480, 0.45, \"Task 3\", fontsize=8, color='blue')\n","plt.rcParams['font.family'] = 'sans-serif'\n","plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']\n","plt.xlabel('$t$')\n","plt.ylabel('Test Error')\n","#plt.legend()\n","plt.savefig(\"my_plot.eps\", dpi=300)\n","plt.show()\n","#files.download(\"my_plot.eps\") # download the figure\n"]},{"cell_type":"markdown","metadata":{"id":"zEreHe4xRAq8"},"source":["d=50,m=1000,n=2500,5000,eta=2,T=200,sigma=0.1/$\\sqrt{d}$\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536},"executionInfo":{"elapsed":240,"status":"error","timestamp":1759805932844,"user":{"displayName":"Hossein Taheri","userId":"10565663845480396106"},"user_tz":420},"id":"MnonxLevf4zo","outputId":"448f3b0f-5c59-44b2-c2e6-5e668925fb0b"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"module functions cannot set METH_CLASS or METH_STATIC","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4020449280.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"]}],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math as m\n","# Define the quadratic activation function\n","import random\n","\n","from google.colab import files\n","\n","seed = 42  # fixed seed\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","\n","class QuadraticActivation(nn.Module):\n","    def forward(self, x):\n","        return (x ** 2)/2\n","\n","# Linear loss defined for +/-1 labels\n","class logloss(nn.Module):\n","    def __init__(self):\n","        super(logloss, self).__init__()\n","\n","    def forward(self, preds, labels):\n","        # Example custom loss: Mean Absolute Error\n","        loss = torch.mean(-preds*labels)\n","        return loss\n","# Define the neural network model\n","class OneHiddenLayerNN(nn.Module):\n","    def __init__(self):\n","        super(OneHiddenLayerNN, self).__init__()\n","        self.fc1 = nn.Linear(dimension, num_neurons)\n","        self.fc2 = nn.Linear(num_neurons, 1, bias=False)\n","        self.fc2.weight.data = torch.from_numpy(np.random.choice([-1/num_neurons**0.5, 1/num_neurons**0.5], size=(1, num_neurons))).float() # second layer weights, fixed\n","        torch.nn.init.normal_(self.fc1.weight,\n","                        mean=0, std=1) # first layer initialization\n","        self.activation = QuadraticActivation()\n","        #nn.init.zeros_(self.fc1.weight)\n","    def forward(self, x):\n","        x = self.activation(self.fc1(x)) # normalized by the number of neuronsß\n","        x = self.fc2(x)\n","        return x\n","\n","#ax.set_facecolor('ghostwhite')\n","\n","  # num_points = 100000 # total number of points in the distribution\n","  # data_points = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, d), replace=True)).float()\n","  # # Generate the labels\n","  # labels = torch.sign(data_points[:, 0] * data_points[:, 1])\n","  # #\n","\n","  # labels = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, 1))).float()\n","\n","\n","\n","# Parameters\n","num_data_points = 15000  # Total number of data points\n","num_train_points = 2500 # training-set size\n","\n","dimension = 50          # Dimension of each data point\n","\n","num_samples_per_class = num_data_points // 2  # Data points per class\n","\n","# Mean vectors for the two classes, generated as tensors\n","#mu_1 = torch.randn(dimension)/dimension  # Mean vector for class 1\n","#mu_2 = torch.randn(dimension)/dimension # Mean vector for class -1\n","mu_1 = torch.zeros(dimension)\n","mu_2= torch.zeros(dimension)\n","mu_1[:2]=1/dimension**0.5\n","mu_2[0]=1/dimension**0.5\n","mu_2[1]=-1/dimension**0.5\n","stdd = 0.1/dimension**0.5\n","# Generate data points for class 1 (label +1)\n","data_class_1_pos = torch.normal(mean=mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_1_neg = torch.normal(mean=-mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_1 = torch.cat((data_class_1_pos, data_class_1_neg), dim=0)\n","labels_class_1 = torch.ones(num_samples_per_class)\n","\n","# Generate data points for class -1 (label -1)\n","data_class_2_pos = torch.normal(mean=mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_2_neg = torch.normal(mean=-mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_2 = torch.cat((data_class_2_pos, data_class_2_neg), dim=0)\n","labels_class_2 = -torch.ones(num_samples_per_class)\n","\n","# Combine the data and labels\n","data = torch.cat((data_class_1, data_class_2), dim=0)\n","labels = torch.cat((labels_class_1, labels_class_2), dim=0)\n","\n","\n","\n","# Split the data into training and test sets\n","train_indices = torch.randperm(num_data_points)[:num_train_points]\n","test_indices = torch.randperm(num_data_points)[num_train_points:]\n","\n","train_data = data[train_indices]\n","train_labels = labels[train_indices]\n","test_data = data[test_indices]\n","test_labels = labels[test_indices]\n","\n","\n","\n","\n","\n","# Parameters\n","\n","# Mean vectors for the two classes, generated as tensors\n","#mu_11 = torch.randn(dimension)/dimension  # Mean vector for class 1\n","#mu_22 = torch.randn(dimension)/dimension  # Mean vector for class -1\n","mu_11 = torch.zeros(dimension)\n","mu_22= torch.zeros(dimension)\n","mu_11[2:4]=1/dimension**0.5\n","mu_22[2]=1/dimension**0.5\n","mu_22[3]=-1/dimension**0.5\n","# Generate data points for class 1 (label +1)\n","data_class_11_pos = torch.normal(mean=mu_11.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_11_neg = torch.normal(mean=-mu_11.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_11 = torch.cat((data_class_11_pos, data_class_11_neg), dim=0)\n","labels_class_11 = torch.ones(num_samples_per_class)\n","\n","# Generate data points for class -1 (label -1)\n","data_class_22_pos = torch.normal(mean=mu_22.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_22_neg = torch.normal(mean=-mu_22.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_22 = torch.cat((data_class_22_pos, data_class_22_neg), dim=0)\n","labels_class_22 = -torch.ones(num_samples_per_class)\n","\n","# Combine the data and labels\n","data2 = torch.cat((data_class_11, data_class_22), dim=0)\n","labels2 = torch.cat((labels_class_11, labels_class_22), dim=0)\n","\n","\n","\n","# Split the data into training and test sets\n","train_indices2 = torch.randperm(num_data_points)[:num_train_points]\n","test_indices2 = torch.randperm(num_data_points)[num_train_points:]\n","\n","train_data2 = data2[train_indices2]\n","train_labels2 = labels2[train_indices2]\n","test_data2 = data2[test_indices2]\n","test_labels2 = labels2[test_indices2]\n","\n","\n","\n","\n","\n","mu_111 = torch.zeros(dimension)\n","mu_222= torch.zeros(dimension)\n","mu_111[4:6]=1/dimension**0.5\n","mu_222[4]=1/dimension**0.5\n","mu_222[5]=-1/dimension**0.5\n","# Generate data points for class 1 (label +1)\n","data_class_111_pos = torch.normal(mean=mu_111.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_111_neg = torch.normal(mean=-mu_111.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_111 = torch.cat((data_class_111_pos, data_class_111_neg), dim=0)\n","labels_class_111 = torch.ones(num_samples_per_class)\n","\n","# Generate data points for class -1 (label -1)\n","data_class_222_pos = torch.normal(mean=mu_222.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_222_neg = torch.normal(mean=-mu_222.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_222 = torch.cat((data_class_222_pos, data_class_222_neg), dim=0)\n","labels_class_222 = -torch.ones(num_samples_per_class)\n","\n","# Combine the data and labels\n","data3 = torch.cat((data_class_111, data_class_222), dim=0)\n","labels3 = torch.cat((labels_class_111, labels_class_222), dim=0)\n","\n","\n","\n","# Split the data into training and test sets\n","train_indices3 = torch.randperm(num_data_points)[:num_train_points]\n","test_indices3 = torch.randperm(num_data_points)[num_train_points:]\n","\n","train_data3 = data3[train_indices3]\n","train_labels3 = labels3[train_indices3]\n","test_data3 = data3[test_indices3]\n","test_labels3 = labels3[test_indices3]\n","\n","\n","\n","\n","num_neurons = 1000\n","learning_rate = 2 # stepsize\n","\n","\n","\n","# Initialize the model, loss function, and optimizer\n","model = OneHiddenLayerNN()\n","criterion = logloss()\n","optimizer = torch.optim.SGD(model.fc1.parameters(), lr=learning_rate)  # Fix the weights of the second layer\n","\n","# Lists to store training and test errors\n","test_errors = []\n","test_errors2 = []\n","test_errors3 = []\n","train_errors = []\n","train_errors2 = []\n","train_errors3 = []\n","num_iterations = round(200)\n","# Training loop\n","mem = 0\n","k= 0\n","tensor0 = model.fc1.weight.clone()\n","weight_norm = []\n","for i in range(3*num_iterations):\n","\n","    with torch.no_grad():\n","      # Calculate test error\n","        test_outputs = model(test_data)\n","        test_predicted_labels = torch.sign(test_outputs.squeeze())\n","        test_error = torch.mean((test_predicted_labels != test_labels).float())\n","        test_errors.append(test_error.item())\n","\n","        test_outputs2 = model(test_data2)\n","        test_predicted_labels2 = torch.sign(test_outputs2.squeeze())\n","        test_error2 = torch.mean((test_predicted_labels2 != test_labels2).float())\n","        test_errors2.append(test_error2.item())\n","\n","        test_outputs3 = model(test_data3)\n","        test_predicted_labels3 = torch.sign(test_outputs3.squeeze())\n","        test_error3 = torch.mean((test_predicted_labels3 != test_labels3).float())\n","        test_errors3.append(test_error3.item())\n","\n","\n","        train_outputs = model(train_data)\n","        train_predicted_labels = torch.sign(train_outputs.squeeze())\n","        train_error = torch.mean((train_predicted_labels != train_labels).float())\n","        train_errors.append(train_error.item())\n","\n","        train_outputs2 = model(train_data2)\n","        train_predicted_labels2 = torch.sign(train_outputs2.squeeze())\n","        train_error2 = torch.mean((train_predicted_labels2 != train_labels2).float())\n","        train_errors2.append(train_error2.item())\n","\n","        train_outputs3 = model(train_data3)\n","        train_predicted_labels3 = torch.sign(train_outputs3.squeeze())\n","        train_error3 = torch.mean((train_predicted_labels3 != train_labels3).float())\n","        train_errors3.append(train_error3.item())\n","\n","\n","        tensor = [i for i in model.fc1.weight]\n","        weight_norm.append(torch.norm(tensor[1]-tensor0[1]))\n","        # Forward pass\n","    if i<num_iterations:\n","        outputs = model(train_data)\n","        loss = criterion(outputs.squeeze(), train_labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    elif i<2*num_iterations:\n","        outputs2 = model(train_data2)\n","        loss = criterion(outputs2.squeeze(), train_labels2)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    else:\n","        outputs3 = model(train_data3)\n","        loss = criterion(outputs3.squeeze(), train_labels3)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","      # Backward pass and optimization\n","\n","    if i%100==0:\n","        print(\"iteration =\",i)\n","        #print(mem)\n","\n","\n","fig4 = plt.figure(\"Figure 1\")\n","fig4, ax = plt.subplots()\n","\n","plt.plot(range(3*num_iterations), train_errors,linewidth =2,color=\"green\")\n","plt.plot(range(3*num_iterations), [np.nan]*num_iterations+train_errors2[num_iterations:],'--',linewidth =2,color=\"red\")\n","plt.plot(range(3*num_iterations), [np.nan]*2*num_iterations+train_errors3[2*num_iterations:],'--',linewidth =2,color=\"blue\")\n","\n","\n","plt.plot([num_iterations]*2,[0,0.8],'--',linewidth =1,color=\"black\")\n","plt.plot([2*num_iterations]*2,[0,0.8],'--',linewidth =1,color=\"black\")\n","ax.set_xlim(left=0)\n","ax.set_ylim(top=0.5)\n","ax.set_ylim(bottom=-.01)\n","ax.text(80, 0.45, \"Task 1\", fontsize=8, color='green')\n","ax.text(280, 0.45, \"Task 2\", fontsize=8, color='red')\n","ax.text(480, 0.45, \"Task 3\", fontsize=8, color='blue')\n","plt.rcParams['font.family'] = 'sans-serif'\n","plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']\n","plt.xlabel('$t$')\n","plt.ylabel('Train/Test Error')\n","#plt.legend()\n","plt.savefig(\"my_plot.eps\", dpi=300)\n","plt.show()\n","files.download(\"my_plot.eps\") # download the figure"]},{"cell_type":"markdown","metadata":{"id":"_0s5etCDJfQK"},"source":["The two codes below can be used for reproducing Fig 4, Fig 5 and Fig 7\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQAniVN-JehF"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math as m\n","# Define the quadratic activation function\n","import random\n","\n","from google.colab import files\n","\n","seed = 42  # fixed seed\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","\n","class QuadraticActivation(nn.Module):\n","    def forward(self, x):\n","        return (x ** 2)/2\n","\n","# Linear loss defined for +/-1 labels\n","class logloss(nn.Module):\n","    def __init__(self):\n","        super(logloss, self).__init__()\n","\n","    def forward(self, preds, labels):\n","        # Example custom loss: Mean Absolute Error\n","        loss = torch.mean(-preds * labels)\n","        return loss\n","# Define the neural network model\n","class OneHiddenLayerNN(nn.Module):\n","    def __init__(self):\n","        super(OneHiddenLayerNN, self).__init__()\n","        self.fc1 = nn.Linear(dimension, num_neurons)\n","        self.fc2 = nn.Linear(num_neurons, 1, bias=False)\n","        self.fc2.weight.data = torch.from_numpy(np.random.choice([-1/num_neurons**0.5, 1/num_neurons**0.5], size=(1, num_neurons))).float() # second layer weights, fixed\n","        torch.nn.init.normal_(self.fc1.weight,\n","                        mean=0, std=1) # first layer initialization\n","        self.activation = QuadraticActivation()\n","        #nn.init.zeros_(self.fc1.weight)\n","    def forward(self, x):\n","        x = self.activation(self.fc1(x)) # normalized by the number of neuronsß\n","        x = self.fc2(x)\n","        return x\n","\n","#ax.set_facecolor('ghostwhite')\n","\n","  # num_points = 100000 # total number of points in the distribution\n","  # data_points = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, d), replace=True)).float()\n","  # # Generate the labels\n","  # labels = torch.sign(data_points[:, 0] * data_points[:, 1])\n","  # #\n","\n","  # labels = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, 1))).float()\n","\n","\n","\n","# Parameters\n","num_data_points = 15000  # Total number of data points\n","num_train_points = 500 # training-set size\n","\n","dimension = 50          # Dimension of each data point\n","\n","num_samples_per_class = num_data_points // 2  # Data points per class\n","\n","# Mean vectors for the two classes, generated as tensors\n","#mu_1 = torch.randn(dimension)/dimension  # Mean vector for class 1\n","#mu_2 = torch.randn(dimension)/dimension # Mean vector for class -1\n","\n","T = 4 # number of tasks\n","train_data = [[] for _ in range(T)]\n","train_labels = [[] for _ in range(T)]\n","test_data = [[] for _ in range(T)]\n","test_labels = [[] for _ in range(T)]\n","\n","for i in range(T):\n","    mu_1 = torch.zeros(dimension)\n","    mu_2 = torch.zeros(dimension)\n","    mu_1[i:i+2]=1/dimension**0.5\n","    mu_2[i]=1/dimension**0.5\n","    mu_2[i+1]=-1/dimension**0.5\n","    stdd = 0.1/dimension**0.5\n","    # Generate data points for class 1 (label +1)\n","    data_class_1_pos = torch.normal(mean=mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","    data_class_1_neg = torch.normal(mean=-mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","    data_class_1 = torch.cat((data_class_1_pos, data_class_1_neg), dim=0)\n","    labels_class_1 = torch.ones(num_samples_per_class)\n","\n","    # Generate data points for class -1 (label -1)\n","    data_class_2_pos = torch.normal(mean=mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","    data_class_2_neg = torch.normal(mean=-mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","    data_class_2 = torch.cat((data_class_2_pos, data_class_2_neg), dim=0)\n","    labels_class_2 = -torch.ones(num_samples_per_class)\n","\n","    # Combine the data and labels\n","    data = torch.cat((data_class_1, data_class_2), dim=0)\n","    labels = torch.cat((labels_class_1, labels_class_2), dim=0)\n","\n","\n","\n","    # Split the data into training and test sets\n","    train_indices = torch.randperm(num_data_points)[:num_train_points]\n","    test_indices = torch.randperm(num_data_points)[num_train_points:]\n","\n","    train_data[i] = data[train_indices]\n","    train_labels[i] = labels[train_indices]\n","    test_data[i] = data[test_indices]\n","    test_labels[i] = labels[test_indices]\n","\n","\n","\n","\n","\n","# Parameters\n","\n","\n","\n","\n","\n","num_neurons = 2000\n","learning_rate = 3 # stepsize\n","\n","\n","\n","# Initialize the model, loss function, and optimizer\n","model = OneHiddenLayerNN()\n","criterion = logloss()\n","optimizer = torch.optim.SGD(model.fc1.parameters(), lr=learning_rate)  # Fix the weights of the second layer\n","\n","# Lists to store training and test errors\n","test_loss = []\n","\n","num_iterations = round(200) # num iterations per task\n","# Training loop\n","train_loss = []\n","for i in range(T*num_iterations):\n","\n","    for k in range(T):\n","        if i<(k+1)*num_iterations and i>=k*num_iterations:\n","            outputs = model(train_data[k])\n","            loss = criterion(outputs.squeeze(), train_labels[k])\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","            if i==(k+1)*num_iterations-1:\n","                 train_outputs = model(train_data[k])\n","                 train_predicted_labels = torch.sign(train_outputs.squeeze())\n","                 train_error = torch.mean((train_predicted_labels != train_labels[k]).float())\n","                 train_loss.append(train_error.item())\n","\n","          # Backward pass and optimization\n","\n","    if i%100==0 and len(train_loss)>=1:\n","        print(\"iteration =\\n\",i)\n","        print(train_loss[-1])\n","\n","\n","fig4 = plt.figure(\"Figure 1\")\n","fig4, ax = plt.subplots()\n","\n","\n","plt.plot(range(T), train_loss,linewidth =2,color=\"green\")\n","\n","#for i in range(T-1):\n"," # plt.plot([(i+1)*num_iterations]*2,[0,0.8],'--',linewidth =1,color=\"black\")\n","ax.set_xlim(left=0)\n","ax.set_ylim(top=0.5)\n","ax.set_ylim(bottom=-.01)\n","\n","plt.rcParams['font.family'] = 'sans-serif'\n","plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']\n","plt.xlabel('$Task$')\n","plt.ylabel('Train Error')\n","#plt.legend()\n","plt.savefig(\"my_plot.eps\", dpi=300)\n","plt.show()\n","files.download(\"my_plot.eps\") # download the figureimport torch\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math as m\n","# Define the quadratic activation function\n","import random\n","\n","from google.colab import files\n","\n","seed = 42  # fixed seed\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","\n","\n","# Linear loss defined for +/-1 labels\n","class logloss(nn.Module):\n","    def __init__(self):\n","        super(logloss, self).__init__()\n","\n","    def forward(self, preds, labels):\n","        # Example custom loss\n","        loss = torch.mean(torch.log(1+torch.exp(-preds*labels)))\n","        return loss\n","# Define the neural network model\n","class OneHiddenLayerNN(nn.Module):\n","    def __init__(self):\n","        super(OneHiddenLayerNN, self).__init__()\n","        self.fc1 = nn.Linear(dimension, num_neurons)\n","        self.fc2 = nn.Linear(num_neurons, 1, bias=False)\n","        self.fc2.weight.data = torch.from_numpy(np.random.choice([-1/num_neurons**0.5, 1/num_neurons**0.5], size=(1, num_neurons))).float() # second layer weights, fixed\n","        torch.nn.init.normal_(self.fc1.weight,\n","                        mean=0, std=1) # first layer initialization\n","        self.activation = nn.GELU()\n","        #nn.init.zeros_(self.fc1.weight)\n","    def forward(self, x):\n","        x = self.activation(self.fc1(x)) # normalized by the number of neuronsß\n","        x = self.fc2(x)\n","        return x\n","\n","#ax.set_facecolor('ghostwhite')\n","\n","  # num_points = 100000 # total number of points in the distribution\n","  # data_points = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, d), replace=True)).float()\n","  # # Generate the labels\n","  # labels = torch.sign(data_points[:, 0] * data_points[:, 1])\n","  # #\n","\n","  # labels = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, 1))).float()\n","\n","\n","\n","# Parameters\n","num_data_points = 15000  # Total number of data points\n","num_train_points = 200 # training-set size\n","\n","dimension = 50          # Dimension of each data point\n","\n","num_samples_per_class = num_data_points // 2  # Data points per class\n","\n","# Mean vectors for the two classes, generated as tensors\n","#mu_1 = torch.randn(dimension)/dimension  # Mean vector for class 1\n","#mu_2 = torch.randn(dimension)/dimension # Mean vector for class -1\n","\n","T = 6 # number of tasks\n","train_data = [[] for _ in range(T)]\n","train_labels = [[] for _ in range(T)]\n","test_data = [[] for _ in range(T)]\n","test_labels = [[] for _ in range(T)]\n","\n","for i in range(T):\n","    mu_1 = torch.zeros(dimension)\n","    mu_2 = torch.zeros(dimension)\n","    mu_1[i:i+2]=1/dimension**0.5\n","    mu_2[i]=1/dimension**0.5\n","    mu_2[i+1]=-1/dimension**0.5\n","    stdd = 0.2/dimension**0.5\n","    # Generate data points for class 1 (label +1)\n","    data_class_1_pos = torch.normal(mean=mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","    data_class_1_neg = torch.normal(mean=-mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","    data_class_1 = torch.cat((data_class_1_pos, data_class_1_neg), dim=0)\n","    labels_class_1 = torch.ones(num_samples_per_class)\n","\n","    # Generate data points for class -1 (label -1)\n","    data_class_2_pos = torch.normal(mean=mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","    data_class_2_neg = torch.normal(mean=-mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","    data_class_2 = torch.cat((data_class_2_pos, data_class_2_neg), dim=0)\n","    labels_class_2 = -torch.ones(num_samples_per_class)\n","\n","    # Combine the data and labels\n","    data = torch.cat((data_class_1, data_class_2), dim=0)\n","    labels = torch.cat((labels_class_1, labels_class_2), dim=0)\n","\n","\n","\n","    # Split the data into training and test sets\n","    train_indices = torch.randperm(num_data_points)[:num_train_points]\n","    test_indices = torch.randperm(num_data_points)[num_train_points:]\n","\n","    train_data[i] = data[train_indices]\n","    train_labels[i] = labels[train_indices]\n","    test_data[i] = data[test_indices]\n","    test_labels[i] = labels[test_indices]\n","\n","\n","\n","\n","\n","# Parameters\n","\n","\n","\n","\n","\n","num_neurons = 2000\n","learning_rate = 30 # stepsize\n","\n","\n","\n","# Initialize the model, loss function, and optimizer\n","model = OneHiddenLayerNN()\n","criterion = logloss()\n","optimizer = torch.optim.SGD(model.fc1.parameters(), lr=learning_rate)  # Fix the weights of the second layer\n","\n","# Lists to store training and test errors\n","test_loss = []\n","\n","num_iterations = round(2000) # num iterations per task\n","# Training loop\n","train_loss = []\n","train_loss_taskwise = []\n","for i in range(T*num_iterations):\n","\n","    for k in range(T):\n","\n","        if i<(k+1)*num_iterations and i>=k*num_iterations:\n","\n","            outputs = model(train_data[k])\n","            loss = criterion(outputs.squeeze(), train_labels[k])\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            train_loss_taskwise.append(loss.item())\n","\n","            if i==(k+1)*num_iterations-1:\n","                with torch.no_grad():\n","\n","                    train_outputs = model(train_data[0])\n","                    loss1 = criterion(train_outputs.squeeze(), train_labels[0])\n","                    train_loss.append(loss1.item())\n","                    print(i)\n","          # Backward pass and optimization\n","\n","\n","\n","\n","fig4 = plt.figure(\"Figure 1\")\n","fig4, ax = plt.subplots()\n","\n","\n","plt.plot(range(T), train_loss,linewidth =2,color=\"green\")\n","#plt.plot(range(T*num_iterations), train_loss_taskwise,linewidth =2,color=\"green\")\n","\n","\n","#for i in range(T-1):\n"," # plt.plot([(i+1)*num_iterations]*2,[0,0.8],'--',linewidth =1,color=\"black\")\n","ax.set_xlim(left=0)\n","ax.set_ylim(top=0.5)\n","ax.set_ylim(bottom=-.001)\n","\n","plt.rcParams['font.family'] = 'sans-serif'\n","plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']\n","plt.xlabel('$Task$')\n","plt.ylabel('Train Loss')\n","#plt.legend()\n","plt.savefig(\"my_plot.eps\", dpi=300)\n","plt.show()\n","#files.download(\"my_plot.eps\") # download the figureimport torch\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536},"id":"N8lGImqLQ2H4","executionInfo":{"status":"error","timestamp":1759805948188,"user_tz":420,"elapsed":122,"user":{"displayName":"Hossein Taheri","userId":"10565663845480396106"}},"outputId":"62acbea0-9d7e-4fbb-a99e-c90884058798"},"execution_count":3,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"module functions cannot set METH_CLASS or METH_STATIC","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1504695314.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"]}]},{"cell_type":"markdown","source":["The codes below can be used for reproducing Fig 4 (Right)"],"metadata":{"id":"7Bb9UtjSNlbA"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math as m\n","# Define the quadratic activation function\n","import random\n","\n","from google.colab import files\n","\n","seed = 2 # fixed seed\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","\n","class QuadraticActivation(nn.Module):\n","    def forward(self, x):\n","        return (x ** 2)/2\n","\n","# Linear loss defined for +/-1 labels\n","class logloss(nn.Module):\n","    def __init__(self):\n","        super(logloss, self).__init__()\n","\n","    def forward(self, preds, labels):\n","        # Example custom loss\n","        #loss = torch.mean(1 - preds * labels)\n","        loss = torch.mean(torch.clamp(1 - preds * labels, min=0))\n","\n","        return loss\n","# Define the neural network model\n","class OneHiddenLayerNN(nn.Module):\n","    def __init__(self):\n","        super(OneHiddenLayerNN, self).__init__()\n","        self.fc1 = nn.Linear(dimension, num_neurons)\n","        self.fc2 = nn.Linear(num_neurons, 1, bias=False)\n","        self.fc2.weight.data = torch.from_numpy(np.random.choice([-1/num_neurons**0.5, 1/num_neurons**0.5], size=(1, num_neurons))).float() # second layer weights, fixed\n","        torch.nn.init.normal_(self.fc1.weight,\n","                        mean=0, std=1) # first layer initialization\n","        self.activation = QuadraticActivation()\n","        #nn.init.zeros_(self.fc1.weight)\n","    def forward(self, x):\n","        x = self.activation(self.fc1(x)) # normalized by the number of neuronsß\n","        x = self.fc2(x)\n","        return x\n","\n","#ax.set_facecolor('ghostwhite')\n","\n","  # num_points = 100000 # total number of points in the distribution\n","  # data_points = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, d), replace=True)).float()\n","  # # Generate the labels\n","  # labels = torch.sign(data_points[:, 0] * data_points[:, 1])\n","  # #\n","\n","  # labels = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, 1))).float()\n","\n","\n","\n","# Parameters\n","num_data_points = 15000  # Total number of data points\n","num_train_points = 200 # training-set size\n","\n","dimension = 50          # Dimension of each data point\n","\n","num_samples_per_class = num_data_points // 2  # Data points per class\n","\n","# Mean vectors for the two classes, generated as tensors\n","#mu_1 = torch.randn(dimension)/dimension  # Mean vector for class 1\n","#mu_2 = torch.randn(dimension)/dimension # Mean vector for class -1\n","\n","T = 10 # number of tasks\n","train_data = [[] for _ in range(T)]\n","train_labels = [[] for _ in range(T)]\n","test_data = [[] for _ in range(T)]\n","test_labels = [[] for _ in range(T)]\n","\n","for i in range(T):\n","    mu_1 = torch.zeros(dimension)\n","    mu_2 = torch.zeros(dimension)\n","    mu_1[i:i+2]=1/dimension**0.5\n","    mu_2[i]=1/dimension**0.5\n","    mu_2[i+1]=-1/dimension**0.5\n","    stdd = 0.1/dimension**0.5\n","    # Generate data points for class 1 (label +1)\n","    data_class_1_pos = torch.normal(mean=mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","    data_class_1_neg = torch.normal(mean=-mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","    data_class_1 = torch.cat((data_class_1_pos, data_class_1_neg), dim=0)\n","    labels_class_1 = torch.ones(num_samples_per_class)\n","\n","    # Generate data points for class -1 (label -1)\n","    data_class_2_pos = torch.normal(mean=mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","    data_class_2_neg = torch.normal(mean=-mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","    data_class_2 = torch.cat((data_class_2_pos, data_class_2_neg), dim=0)\n","    labels_class_2 = -torch.ones(num_samples_per_class)\n","\n","    # Combine the data and labels\n","    data = torch.cat((data_class_1, data_class_2), dim=0)\n","    labels = torch.cat((labels_class_1, labels_class_2), dim=0)\n","\n","\n","\n","    # Split the data into training and test sets\n","    train_indices = torch.randperm(num_data_points)[:num_train_points]\n","    test_indices = torch.randperm(num_data_points)[num_train_points:]\n","\n","    train_data[i] = data[train_indices]\n","    train_labels[i] = labels[train_indices]\n","    test_data[i] = data[test_indices]\n","    test_labels[i] = labels[test_indices]\n","\n","\n","\n","\n","\n","# Parameters\n","\n","\n","\n","\n","\n","num_neurons = 1000\n","learning_rate = 4 # stepsize\n","\n","\n","\n","# Initialize the model, loss function, and optimizer\n","model = OneHiddenLayerNN()\n","criterion = logloss()\n","optimizer = torch.optim.SGD(model.fc1.parameters(), lr=learning_rate)  # Fix the weights of the second layer\n","\n","# Lists to store training and test errors\n","test_loss = []\n","\n","num_iterations = round(2000) # num iterations per task\n","# Training loop\n","train_loss = []\n","train_loss_taskwise = []\n","for i in range(T*num_iterations):\n","\n","    for k in range(T):\n","        if i<(k+1)*num_iterations and i>=k*num_iterations:\n","            outputs = model(train_data[k])\n","            loss = criterion(outputs.squeeze(), train_labels[k])\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            train_loss_taskwise.append(loss.item())\n","\n","            if i==(k+1)*num_iterations-1:\n","                 train_outputs = model(train_data[0])\n","                 loss1 = criterion(train_outputs.squeeze(), train_labels[0])\n","\n","                 #train_predicted_labels = torch.sign(train_outputs.squeeze())\n","                 #train_error = torch.mean((train_predicted_labels != train_labels[0]).float())\n","                 train_loss.append(loss1.item())\n","                 print(i)\n","          # Backward pass and optimization\n","\n","\n","\n","\n","fig4 = plt.figure(\"Figure 1\")\n","fig4, ax = plt.subplots()\n","\n","\n","plt.plot(range(T), train_loss,linewidth =2,color=\"green\")\n","#plt.plot(range(T*num_iterations), train_loss_taskwise,linewidth =2,color=\"green\")\n","\n","\n","#for i in range(T-1):\n"," # plt.plot([(i+1)*num_iterations]*2,[0,0.8],'--',linewidth =1,color=\"black\")\n","ax.set_xlim(left=0)\n","ax.set_ylim(top=0.5)\n","ax.set_ylim(bottom=-.001)\n","\n","plt.rcParams['font.family'] = 'sans-serif'\n","plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']\n","plt.xlabel('$Task$')\n","plt.ylabel('Train Error')\n","#plt.legend()\n","plt.savefig(\"my_plot.eps\", dpi=500)\n","plt.show()\n","#files.download(\"my_plot.eps\") # download the figureimport torch\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536},"id":"5MqMHZu3hmKG","executionInfo":{"status":"error","timestamp":1759805955087,"user_tz":420,"elapsed":45,"user":{"displayName":"Hossein Taheri","userId":"10565663845480396106"}},"outputId":"2094c20a-0b00-4233-9f8f-215ac5d7da90"},"execution_count":4,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"module functions cannot set METH_CLASS or METH_STATIC","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2934264220.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"]}]},{"cell_type":"markdown","metadata":{"id":"NkAY9tbyCeo5"},"source":["same experiment as above but with logistic_loss and GeLU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75OvO1rfvK9P","outputId":"91f8b9ef-93df-41f9-c6c1-784328777e06"},"outputs":[{"output_type":"stream","name":"stdout","text":["iteration = 0\n","iteration = 100\n","iteration = 200\n","iteration = 300\n","iteration = 400\n","iteration = 500\n","iteration = 600\n","iteration = 700\n","iteration = 800\n","iteration = 900\n","iteration = 1000\n","iteration = 1100\n"]}],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math as m\n","# Define the quadratic activation function\n","import random\n","\n","from google.colab import files\n","\n","seed = 42  # fixed seed\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","\n","# Linear loss defined for +/-1 labels\n","class logloss(nn.Module):\n","    def __init__(self):\n","        super(logloss, self).__init__()\n","\n","    def forward(self, preds, labels):\n","        # Example custom loss: Mean Absolute Error\n","        loss = torch.mean(torch.log(1+torch.exp(-preds*labels)))\n","        return loss\n","# Define the neural network model\n","class OneHiddenLayerNN(nn.Module):\n","    def __init__(self):\n","        super(OneHiddenLayerNN, self).__init__()\n","        self.fc1 = nn.Linear(dimension, num_neurons)\n","        self.fc2 = nn.Linear(num_neurons, 1, bias=False)\n","        self.fc2.weight.data = torch.from_numpy(np.random.choice([-1/num_neurons**0.5, 1/num_neurons**0.5], size=(1, num_neurons))).float() # second layer weights, fixed\n","        torch.nn.init.normal_(self.fc1.weight,\n","                        mean=0, std=1) # first layer initialization\n","        self.activation = nn.ReLU()\n","        #nn.init.zeros_(self.fc1.weight)\n","    def forward(self, x):\n","        x = self.activation(self.fc1(x)) # normalized by the number of neuronsß\n","        x = self.fc2(x)\n","        return x\n","\n","#ax.set_facecolor('ghostwhite')\n","\n","  # num_points = 100000 # total number of points in the distribution\n","  # data_points = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, d), replace=True)).float()\n","  # # Generate the labels\n","  # labels = torch.sign(data_points[:, 0] * data_points[:, 1])\n","  # #\n","\n","  # labels = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, 1))).float()\n","\n","\n","\n","# Parameters\n","num_data_points = 15000  # Total number of data points\n","num_train_points = 400 # training-set size\n","\n","dimension = 50          # Dimension of each data point\n","\n","num_samples_per_class = num_data_points // 2  # Data points per class\n","\n","# Mean vectors for the two classes, generated as tensors\n","#mu_1 = torch.randn(dimension)/dimension  # Mean vector for class 1\n","#mu_2 = torch.randn(dimension)/dimension # Mean vector for class -1\n","mu_1 = torch.zeros(dimension)\n","mu_2= torch.zeros(dimension)\n","mu_1[:2]=1/dimension**0.5\n","mu_2[0]=1/dimension**0.5\n","mu_2[1]=-1/dimension**0.5\n","stdd = 0.1/dimension**0.5\n","# Generate data points for class 1 (label +1)\n","data_class_1_pos = torch.normal(mean=mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_1_neg = torch.normal(mean=-mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_1 = torch.cat((data_class_1_pos, data_class_1_neg), dim=0)\n","labels_class_1 = torch.ones(num_samples_per_class)\n","\n","# Generate data points for class -1 (label -1)\n","data_class_2_pos = torch.normal(mean=mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_2_neg = torch.normal(mean=-mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_2 = torch.cat((data_class_2_pos, data_class_2_neg), dim=0)\n","labels_class_2 = -torch.ones(num_samples_per_class)\n","\n","# Combine the data and labels\n","data = torch.cat((data_class_1, data_class_2), dim=0)\n","labels = torch.cat((labels_class_1, labels_class_2), dim=0)\n","\n","\n","\n","# Split the data into training and test sets\n","train_indices = torch.randperm(num_data_points)[:num_train_points]\n","test_indices = torch.randperm(num_data_points)[num_train_points:]\n","\n","train_data = data[train_indices]\n","train_labels = labels[train_indices]\n","test_data = data[test_indices]\n","test_labels = labels[test_indices]\n","\n","\n","\n","\n","\n","# Parameters\n","\n","# Mean vectors for the two classes, generated as tensors\n","#mu_11 = torch.randn(dimension)/dimension  # Mean vector for class 1\n","#mu_22 = torch.randn(dimension)/dimension  # Mean vector for class -1\n","mu_11 = torch.zeros(dimension)\n","mu_22= torch.zeros(dimension)\n","mu_11[2:4]=1/dimension**0.5\n","mu_22[2]=1/dimension**0.5\n","mu_22[3]=-1/dimension**0.5\n","\n","\n","# Generate data points for class 1 (label +1)\n","data_class_11_pos = torch.normal(mean=mu_11.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_11_neg = torch.normal(mean=-mu_11.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_11 = torch.cat((data_class_11_pos, data_class_11_neg), dim=0)\n","labels_class_11 = torch.ones(num_samples_per_class)\n","\n","# Generate data points for class -1 (label -1)\n","data_class_22_pos = torch.normal(mean=mu_22.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_22_neg = torch.normal(mean=-mu_22.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_22 = torch.cat((data_class_22_pos, data_class_22_neg), dim=0)\n","labels_class_22 = -torch.ones(num_samples_per_class)\n","\n","# Combine the data and labels\n","data2 = torch.cat((data_class_11, data_class_22), dim=0)\n","labels2 = torch.cat((labels_class_11, labels_class_22), dim=0)\n","\n","\n","num_train_points = 400\n","# Split the data into training and test sets\n","train_indices2 = torch.randperm(num_data_points)[:num_train_points]\n","test_indices2 = torch.randperm(num_data_points)[num_train_points:]\n","\n","train_data2 = data2[train_indices2]\n","train_labels2 = labels2[train_indices2]\n","test_data2 = data2[test_indices2]\n","test_labels2 = labels2[test_indices2]\n","\n","\n","\n","\n","\n","mu_111 = torch.zeros(dimension)\n","mu_222= torch.zeros(dimension)\n","mu_111[4:6]=1/dimension**0.5\n","mu_222[4]=1/dimension**0.5\n","mu_222[5]=-1/dimension**0.5\n","# Generate data points for class 1 (label +1)\n","data_class_111_pos = torch.normal(mean=mu_111.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_111_neg = torch.normal(mean=-mu_111.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_111 = torch.cat((data_class_111_pos, data_class_111_neg), dim=0)\n","labels_class_111 = torch.ones(num_samples_per_class)\n","\n","# Generate data points for class -1 (label -1)\n","data_class_222_pos = torch.normal(mean=mu_222.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_222_neg = torch.normal(mean=-mu_222.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_222 = torch.cat((data_class_222_pos, data_class_222_neg), dim=0)\n","labels_class_222 = -torch.ones(num_samples_per_class)\n","\n","# Combine the data and labels\n","data3 = torch.cat((data_class_111, data_class_222), dim=0)\n","labels3 = torch.cat((labels_class_111, labels_class_222), dim=0)\n","\n","\n","\n","# Split the data into training and test sets\n","train_indices3 = torch.randperm(num_data_points)[:num_train_points]\n","test_indices3 = torch.randperm(num_data_points)[num_train_points:]\n","\n","train_data3 = data3[train_indices3]\n","train_labels3 = labels3[train_indices3]\n","test_data3 = data3[test_indices3]\n","test_labels3 = labels3[test_indices3]\n","\n","\n","\n","\n","num_neurons = 1000\n","learning_rate = .3 # stepsize\n","\n","\n","\n","# Initialize the model, loss function, and optimizer\n","model = OneHiddenLayerNN()\n","criterion = logloss()\n","optimizer = torch.optim.SGD(model.fc1.parameters(), lr=learning_rate)  # Fix the weights of the second layer\n","\n","# Lists to store training and test errors\n","test_errors = []\n","test_errors2 = []\n","test_errors3 = []\n","train_errors = []\n","train_errors2 = []\n","train_errors3 = []\n","num_iterations = round(400)\n","# Training loop\n","mem = 0\n","k= 0\n","tensor0 = model.fc1.weight.clone()\n","weight_norm = []\n","for i in range(3*num_iterations):\n","\n","    with torch.no_grad():\n","      # Calculate test error\n","        #test_outputs = model(test_data)\n","        #test_predicted_labels = torch.sign(test_outputs.squeeze())\n","        #test_error = torch.mean((test_predicted_labels != test_labels).float())\n","        #test_errors.append(test_error.item())\n","\n","        #test_outputs2 = model(test_data2)\n","        #test_predicted_labels2 = torch.sign(test_outputs2.squeeze())\n","        #test_error2 = torch.mean((test_predicted_labels2 != test_labels2).float())\n","        #test_errors2.append(test_error2.item())\n","\n","        #test_outputs3 = model(test_data3)\n","        #test_predicted_labels3 = torch.sign(test_outputs3.squeeze())\n","        #test_error3 = torch.mean((test_predicted_labels3 != test_labels3).float())\n","        #test_errors3.append(test_error3.item())\n","\n","\n","        train_outputs = model(train_data)\n","        train_predicted_labels = torch.sign(train_outputs.squeeze())\n","        train_error = torch.mean((train_predicted_labels != train_labels).float())\n","        train_errors.append(train_error.item())\n","\n","        train_outputs2 = model(train_data2)\n","        train_predicted_labels2 = torch.sign(train_outputs2.squeeze())\n","        train_error2 = torch.mean((train_predicted_labels2 != train_labels2).float())\n","        train_errors2.append(train_error2.item())\n","\n","        train_outputs3 = model(train_data3)\n","        train_predicted_labels3 = torch.sign(train_outputs3.squeeze())\n","        train_error3 = torch.mean((train_predicted_labels3 != train_labels3).float())\n","        train_errors3.append(train_error3.item())\n","\n","\n","        tensor = [i for i in model.fc1.weight]\n","        weight_norm.append(torch.norm(tensor[1]-tensor0[1]))\n","        # Forward pass\n","    if i<num_iterations:\n","        outputs = model(train_data)\n","        loss = criterion(outputs.squeeze(), train_labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    elif i<2*num_iterations:\n","        outputs2 = model(train_data2)\n","        loss = criterion(outputs2.squeeze(), train_labels2)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    else:\n","        outputs3 = model(train_data3)\n","        loss = criterion(outputs3.squeeze(), train_labels3)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","      # Backward pass and optimization\n","\n","    if i%100==0:\n","        print(\"iteration =\",i)\n","        #print(mem)\n","\n","\n","fig4 = plt.figure(\"Figure 1\")\n","fig4, ax = plt.subplots()\n","#plt.plot(range(3*num_iterations), test_errors,linewidth =2,color=\"green\")\n","#plt.plot(range(3*num_iterations), [np.nan]*num_iterations+test_errors2[num_iterations:],linewidth =2,color=\"red\")\n","#plt.plot(range(3*num_iterations), [np.nan]*2*num_iterations+test_errors3[2*num_iterations:],linewidth =2,color=\"blue\")\n","\n","plt.plot(range(3*num_iterations), train_errors,linewidth =2,color=\"green\")\n","plt.plot(range(3*num_iterations), [np.nan]*num_iterations+train_errors2[num_iterations:],linewidth =2,color=\"red\")\n","plt.plot(range(3*num_iterations), [np.nan]*2*num_iterations+train_errors3[2*num_iterations:],linewidth =2,color=\"blue\")\n","\n","\n","plt.plot([num_iterations]*2,[0,0.8],'--',linewidth =1,color=\"black\")\n","plt.plot([2*num_iterations]*2,[0,0.8],'--',linewidth =1,color=\"black\")\n","ax.set_xlim(left=0)\n","ax.set_ylim(top=0.5)\n","ax.set_ylim(bottom=-.01)\n","ax.text(180, 0.45, \"Task 1\", fontsize=8, color='green')\n","ax.text(180, 0.42, \"n=1000\", fontsize=8, color='green')\n","ax.text(580, 0.45, \"Task 2\", fontsize=8, color='red')\n","ax.text(580, 0.42, \"n=1000\", fontsize=8, color='red')\n","ax.text(980, 0.45, \"Task 3\", fontsize=8, color='blue')\n","ax.text(980, 0.42, \"n=1000\", fontsize=8, color='blue')\n","plt.rcParams['font.family'] = 'sans-serif'\n","plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']\n","plt.xlabel('$t$')\n","plt.ylabel('Train Error')\n","#plt.legend()\n","plt.savefig(\"my_plot.eps\", dpi=300)\n","plt.show()\n","files.download(\"my_plot.eps\") # download the figure"]},{"cell_type":"markdown","metadata":{"id":"eh46tNR4B4cE"},"source":["d=75,m=1000,n=5000,eta=5,T=200,sigma=0.15/$\\sqrt{d}$\n","\n","\n"]},{"cell_type":"markdown","source":["The codes below can be used for reproducing Fig 6."],"metadata":{"id":"PgObxTSXIu8B"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":574},"executionInfo":{"elapsed":947981,"status":"ok","timestamp":1755133055281,"user":{"displayName":"Hossein Taheri","userId":"16225445648060477440"},"user_tz":420},"id":"M3RkYJfky7YW","outputId":"1ec368af-151f-4666-b107-e97ae5fd9a06"},"outputs":[{"name":"stdout","output_type":"stream","text":["iteration = 0\n","iteration = 100\n","iteration = 200\n","iteration = 300\n","iteration = 400\n","iteration = 500\n"]},{"data":{"text/plain":["<Figure size 640x480 with 0 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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\n","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":["download(\"download_ba01a5fa-7273-4c5b-a778-ffed60f9f122\", \"my_plot.eps\", 25933)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math as m\n","import random\n","from google.colab import files\n","\n","seed = 2  # fixed seed\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","# Define the quadratic activation function\n","\n","\n","class QuadraticActivation(nn.Module):\n","    def forward(self, x):\n","        return (x ** 2)/2\n","\n","# Linear loss defined for +/-1 labels\n","class logloss(nn.Module):\n","    def __init__(self):\n","        super(logloss, self).__init__()\n","\n","    def forward(self, preds, labels):\n","        # Example custom loss: Mean Absolute Error\n","        loss = torch.mean(-preds*labels)\n","        return loss\n","# Define the neural network model\n","class OneHiddenLayerNN(nn.Module):\n","    def __init__(self):\n","        super(OneHiddenLayerNN, self).__init__()\n","        self.fc1 = nn.Linear(dimension, num_neurons)\n","        self.fc2 = nn.Linear(num_neurons, 1, bias=False)\n","        self.fc2.weight.data = torch.from_numpy(np.random.choice([-1/num_neurons**0.5, 1/num_neurons**0.5], size=(1, num_neurons))).float() # second layer weights, fixed\n","        torch.nn.init.normal_(self.fc1.weight,\n","                        mean=0, std=1) # first layer initialization\n","        self.activation = QuadraticActivation()\n","        #nn.init.zeros_(self.fc1.weight)\n","    def forward(self, x):\n","        x = self.activation(self.fc1(x)) # normalized by the number of neuronsß\n","        x = self.fc2(x)\n","        return x\n","\n","#ax.set_facecolor('ghostwhite')\n","\n","  # num_points = 100000 # total number of points in the distribution\n","  # data_points = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, d), replace=True)).float()\n","  # # Generate the labels\n","  # labels = torch.sign(data_points[:, 0] * data_points[:, 1])\n","  # #\n","\n","  # labels = torch.from_numpy(np.random.choice([-1, 1], size=(num_points, 1))).float()\n","\n","\n","\n","# Parameters\n","num_data_points = 10000  # Total number of data points\n","num_train_points = 5000 # training-set size\n","\n","dimension = 75          # Dimension of each data point\n","\n","num_samples_per_class = num_data_points // 2  # Data points per class\n","\n","# Mean vectors for the two classes, generated as tensors\n","#mu_1 = torch.randn(dimension)/dimension  # Mean vector for class 1\n","#mu_2 = torch.randn(dimension)/dimension # Mean vector for class -1\n","mu_1 = torch.zeros(dimension)\n","mu_2= torch.zeros(dimension)\n","mu_1[:2]=1/dimension**0.5\n","mu_2[0]=1/dimension**0.5\n","mu_2[1]=-1/dimension**0.5\n","stdd = 0.15/dimension**0.5\n","# Generate data points for class 1 (label +1)\n","data_class_1_pos = torch.normal(mean=mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_1_neg = torch.normal(mean=-mu_1.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_1 = torch.cat((data_class_1_pos, data_class_1_neg), dim=0)\n","labels_class_1 = torch.ones(num_samples_per_class)\n","\n","# Generate data points for class -1 (label -1)\n","data_class_2_pos = torch.normal(mean=mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_2_neg = torch.normal(mean=-mu_2.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_2 = torch.cat((data_class_2_pos, data_class_2_neg), dim=0)\n","labels_class_2 = -torch.ones(num_samples_per_class)\n","\n","# Combine the data and labels\n","data = torch.cat((data_class_1, data_class_2), dim=0)\n","labels = torch.cat((labels_class_1, labels_class_2), dim=0)\n","\n","\n","\n","# Split the data into training and test sets\n","train_indices = torch.randperm(num_data_points)[:num_train_points]\n","test_indices = torch.randperm(num_data_points)[num_train_points:]\n","\n","train_data = data[train_indices]\n","train_labels = labels[train_indices]\n","test_data = data[test_indices]\n","test_labels = labels[test_indices]\n","\n","\n","\n","\n","\n","# Parameters\n","\n","# Mean vectors for the two classes, generated as tensors\n","#mu_11 = torch.randn(dimension)/dimension  # Mean vector for class 1\n","#mu_22 = torch.randn(dimension)/dimension  # Mean vector for class -1\n","mu_11 = torch.zeros(dimension)\n","mu_22= torch.zeros(dimension)\n","mu_11[2:4]=1/dimension**0.5\n","mu_22[2]=1/dimension**0.5\n","mu_22[3]=-1/dimension**0.5\n","# Generate data points for class 1 (label +1)\n","data_class_11_pos = torch.normal(mean=mu_11.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_11_neg = torch.normal(mean=-mu_11.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_11 = torch.cat((data_class_11_pos, data_class_11_neg), dim=0)\n","labels_class_11 = torch.ones(num_samples_per_class)\n","\n","# Generate data points for class -1 (label -1)\n","data_class_22_pos = torch.normal(mean=mu_22.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_22_neg = torch.normal(mean=-mu_22.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_22 = torch.cat((data_class_22_pos, data_class_22_neg), dim=0)\n","labels_class_22 = -torch.ones(num_samples_per_class)\n","\n","# Combine the data and labels\n","data2 = torch.cat((data_class_11, data_class_22), dim=0)\n","labels2 = torch.cat((labels_class_11, labels_class_22), dim=0)\n","\n","\n","\n","# Split the data into training and test sets\n","train_indices2 = torch.randperm(num_data_points)[:num_train_points]\n","test_indices2 = torch.randperm(num_data_points)[num_train_points:]\n","\n","train_data2 = data2[train_indices2]\n","train_labels2 = labels2[train_indices2]\n","test_data2 = data2[test_indices2]\n","test_labels2 = labels2[test_indices2]\n","\n","\n","\n","\n","\n","mu_111 = torch.zeros(dimension)\n","mu_222= torch.zeros(dimension)\n","mu_111[4:6]=1/dimension**0.5\n","mu_222[4]=1/dimension**0.5\n","mu_222[5]=-1/dimension**0.5\n","# Generate data points for class 1 (label +1)\n","data_class_111_pos = torch.normal(mean=mu_111.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_111_neg = torch.normal(mean=-mu_111.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_111 = torch.cat((data_class_111_pos, data_class_111_neg), dim=0)\n","labels_class_111 = torch.ones(num_samples_per_class)\n","\n","# Generate data points for class -1 (label -1)\n","data_class_222_pos = torch.normal(mean=mu_222.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_222_neg = torch.normal(mean=-mu_222.expand(num_samples_per_class // 2, dimension), std=stdd)\n","data_class_222 = torch.cat((data_class_222_pos, data_class_222_neg), dim=0)\n","labels_class_222 = -torch.ones(num_samples_per_class)\n","\n","# Combine the data and labels\n","data3 = torch.cat((data_class_111, data_class_222), dim=0)\n","labels3 = torch.cat((labels_class_111, labels_class_222), dim=0)\n","\n","\n","\n","# Split the data into training and test sets\n","train_indices3 = torch.randperm(num_data_points)[:num_train_points]\n","test_indices3 = torch.randperm(num_data_points)[num_train_points:]\n","\n","train_data3 = data3[train_indices3]\n","train_labels3 = labels3[train_indices3]\n","test_data3 = data3[test_indices3]\n","test_labels3 = labels3[test_indices3]\n","\n","\n","\n","\n","num_neurons = 10000\n","learning_rate = 5 # stepsize\n","\n","\n","\n","# Initialize the model, loss function, and optimizer\n","model = OneHiddenLayerNN()\n","criterion = logloss()\n","optimizer = torch.optim.SGD(model.fc1.parameters(), lr=learning_rate)  # Fix the weights of the second layer\n","\n","# Lists to store training and test errors\n","test_errors = []\n","test_errors2 = []\n","test_errors3 = []\n","train_errors = []\n","train_errors2 = []\n","train_errors3 = []\n","num_iterations = round(200)\n","# Training loop\n","mem = 0\n","k= 0\n","tensor0 = model.fc1.weight.clone()\n","weight_norm = []\n","for i in range(3*num_iterations):\n","\n","    with torch.no_grad():\n","      # Calculate test error\n","        test_outputs = model(test_data)\n","        test_predicted_labels = torch.sign(test_outputs.squeeze())\n","        test_error = torch.mean((test_predicted_labels != test_labels).float())\n","        test_errors.append(test_error.item())\n","\n","        test_outputs2 = model(test_data2)\n","        test_predicted_labels2 = torch.sign(test_outputs2.squeeze())\n","        test_error2 = torch.mean((test_predicted_labels2 != test_labels2).float())\n","        test_errors2.append(test_error2.item())\n","\n","        test_outputs3 = model(test_data3)\n","        test_predicted_labels3 = torch.sign(test_outputs3.squeeze())\n","        test_error3 = torch.mean((test_predicted_labels3 != test_labels3).float())\n","        test_errors3.append(test_error3.item())\n","\n","\n","\n","    if i<num_iterations:\n","        outputs = model(train_data)\n","        loss = criterion(outputs.squeeze(), train_labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    elif i<2*num_iterations:\n","        outputs2 = model(train_data2)\n","        loss = criterion(outputs2.squeeze(), train_labels2)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    else:\n","        outputs3 = model(train_data3)\n","        loss = criterion(outputs3.squeeze(), train_labels3)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","      # Backward pass and optimization\n","\n","    if i%100==0:\n","        print(\"iteration =\",i)\n","        #print(mem)\n","\n","\n","fig4 = plt.figure(\"Figure 1\")\n","fig4, ax = plt.subplots()\n","plt.plot(range(3*num_iterations), test_errors,linewidth =2,color=\"green\")\n","plt.plot(range(3*num_iterations), [np.nan]*num_iterations+test_errors2[num_iterations:],linewidth =2,color=\"red\")\n","plt.plot(range(3*num_iterations), [np.nan]*2*num_iterations+test_errors3[2*num_iterations:],linewidth =2,color=\"blue\")\n","\n","#plt.plot(range(3*num_iterations), train_errors,'--',linewidth =2,color=\"green\")\n","#plt.plot(range(3*num_iterations), [np.nan]*num_iterations+train_errors2[num_iterations:],'--',linewidth =2,color=\"red\")\n","#plt.plot(range(3*num_iterations), [np.nan]*2*num_iterations+train_errors3[2*num_iterations:],'--',linewidth =2,color=\"blue\")\n","\n","plt.plot([num_iterations]*2,[0,0.8],'--',linewidth =1,color=\"black\")\n","plt.plot([2*num_iterations]*2,[0,0.8],'--',linewidth =1,color=\"black\")\n","ax.set_xlim(left=0)\n","ax.set_ylim(top=0.5)\n","ax.set_ylim(bottom=-.01)\n","ax.text(80, 0.45, \"Task 1\", fontsize=8, color='green')\n","ax.text(280, 0.45, \"Task 2\", fontsize=8, color='red')\n","ax.text(480, 0.45, \"Task 3\", fontsize=8, color='blue')\n","plt.rcParams['font.family'] = 'sans-serif'\n","plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']\n","plt.xlabel('$t$')\n","plt.ylabel('Test Error')\n","#plt.legend()\n","plt.savefig(\"my_plot.eps\", dpi=300)\n","plt.show()\n","\n","files.download(\"my_plot.eps\") # download the figure"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":360},"executionInfo":{"elapsed":12,"status":"error","timestamp":1759805982797,"user":{"displayName":"Hossein Taheri","userId":"10565663845480396106"},"user_tz":420},"id":"fTrZIZlG2BIs","outputId":"64281009-8943-460e-f520-68422a10025d"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'plt' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3453835355.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Figure 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"green\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtest_errors2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtest_errors3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"]}],"source":["fig4 = plt.figure(\"Figure 1\")\n","fig4, ax = plt.subplots()\n","plt.plot(range(3*num_iterations), test_errors,linewidth =2,color=\"green\")\n","plt.plot(range(3*num_iterations), [np.nan]*num_iterations+test_errors2[num_iterations:],linewidth =2,color=\"red\")\n","plt.plot(range(3*num_iterations), [np.nan]*2*num_iterations+test_errors3[2*num_iterations:],linewidth =2,color=\"blue\")\n","\n","#plt.plot(range(3*num_iterations), train_errors,'--',linewidth =2,color=\"green\")\n","#plt.plot(range(3*num_iterations), [np.nan]*num_iterations+train_errors2[num_iterations:],'--',linewidth =2,color=\"red\")\n","#plt.plot(range(3*num_iterations), [np.nan]*2*num_iterations+train_errors3[2*num_iterations:],'--',linewidth =2,color=\"blue\")\n","\n","plt.plot([num_iterations]*2,[0,0.8],'--',linewidth =1,color=\"black\")\n","plt.plot([2*num_iterations]*2,[0,0.8],'--',linewidth =1,color=\"black\")\n","ax.set_xlim(left=0)\n","ax.set_ylim(top=0.5)\n","ax.set_ylim(bottom=-.01)\n","ax.text(80, 0.45, \"Task 1\", fontsize=8, color='green')\n","ax.text(280, 0.45, \"Task 2\", fontsize=8, color='red')\n","ax.text(480, 0.45, \"Task 3\", fontsize=8, color='blue')\n","plt.rcParams['font.family'] = 'sans-serif'\n","plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']\n","plt.xlabel('$t$')\n","plt.ylabel('Test Error')\n","#plt.legend()\n","plt.savefig(\"my_plot.eps\", dpi=300)\n","plt.show()\n","\n","files.download(\"my_plot.eps\") # download the figure"]}],"metadata":{"colab":{"provenance":[{"file_id":"1iDIXJQ-Xdh_qUPqY5XgncKzOWCmm3AI3","timestamp":1756233760883}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}