{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5spEQjkZk8aSVqwCRnuBS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536},"id":"eiZ5t0TKom5l","executionInfo":{"status":"error","timestamp":1759806428432,"user_tz":420,"elapsed":24,"user":{"displayName":"Hossein Taheri","userId":"10565663845480396106"}},"outputId":"09352b77-6470-4336-a06a-f12ed2b96701"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"module functions cannot set METH_CLASS or METH_STATIC","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2873276350.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import Subset\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","seed = 2  # fixed seed\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","# -------------------\n","# Setup\n","# -------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","T = 50          # number of gradient steps per task\n","hidden_dim = 1000\n","lr = 0.01\n","\n","# Number of data points for each task\n","num_samples_task1 = 100\n","num_samples_task2 = 50\n","\n","# -------------------\n","# Logistic loss\n","# -------------------\n","class LogisticLoss(nn.Module):\n","    def forward(self, preds, labels):\n","        return torch.mean(torch.log(1 + torch.exp(-labels * preds)))\n","\n","# -------------------\n","# Dataset preparation\n","# -------------------\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","train_dataset = torchvision.datasets.FashionMNIST(\n","    root=\"./data\", train=True, download=True, transform=transform\n",")\n","\n","def get_binary_task(dataset, class_a, class_b, num_samples):\n","    idx = np.where((np.array(dataset.targets) == class_a) |\n","                   (np.array(dataset.targets) == class_b))[0]\n","    # randomly select num_samples\n","    idx = np.random.choice(idx, size=num_samples, replace=False)\n","    subset = Subset(dataset, idx)\n","    labels = dataset.targets[idx].clone()\n","    labels[labels == class_a] = 1\n","    labels[labels == class_b] = -1\n","    return subset, labels.to(device)\n","\n","# Task 1: Dress (3) vs Ankle boot (9)\n","task1_data, task1_labels = get_binary_task(train_dataset, 6, 7, num_samples_task1)\n","X1_full = torch.stack([task1_data[i][0] for i in range(len(task1_data))]).to(device)\n","y1_full = task1_labels\n","\n","# Task 2: T-shirt/top (0) vs Coat (4)\n","task2_data, task2_labels = get_binary_task(train_dataset, 8,9, num_samples_task2)\n","X2_full = torch.stack([task2_data[i][0] for i in range(len(task2_data))]).to(device)\n","y2_full = task2_labels\n","\n","# -------------------\n","# Model: 1 hidden layer MLP\n","# -------------------\n","class MLP(nn.Module):\n","    def __init__(self, input_dim=28*28, hidden_dim=256):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.act = nn.GELU()\n","        self.fc2 = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)\n","        x = self.act(self.fc1(x))\n","        return self.fc2(x).squeeze(-1)\n","\n","model = MLP(hidden_dim=hidden_dim).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=lr)\n","criterion = LogisticLoss()\n","\n","# -------------------\n","# Full-batch training loop\n","# -------------------\n","per_task_losses = [[], []]\n","first_task_losses = []\n","\n","tasks = [(X1_full, y1_full), (X2_full, y2_full)]\n","\n","for task_id, (X_full, y_full) in enumerate(tasks):\n","    print(f\"Training Task {task_id+1} with full-batch GD ({X_full.size(0)} samples)\")\n","    for t in range(T):\n","        optimizer.zero_grad()\n","        logits = model(X_full)\n","        loss = criterion(logits, y_full)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Evaluate first task loss\n","        with torch.no_grad():\n","            logits_first = model(X1_full)\n","            loss_first = criterion(logits_first, y1_full)\n","            first_task_losses.append(loss_first.item())\n","\n","            loss_current = criterion(logits, y_full)\n","            per_task_losses[task_id].append(loss_current.item())\n","\n","        if t % 5 == 0:\n","            print(f\"Iteration {t}: Task {task_id+1} loss = {loss_current.item():.4f}, \"\n","                  f\"First task loss = {loss_first.item():.4f}\")\n","\n","print(\"Final per-task losses:\", [losses[-1] for losses in per_task_losses])\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import Subset\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import random\n","seed = 2  # fixed seed\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","class HingeLoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, preds, labels):\n","        return torch.mean(F.relu(1 - preds * labels))\n","\n","# -------------------\n","# Setup\n","# -------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","T = 200          # number of gradient steps per task\n","hidden_dim = 500\n","lr = 0.001\n","\n","# Number of data points per task\n","num_samples_task1 = 50\n","num_samples_task2 = 100\n","\n","# Number of experiments\n","num_experiments = 15\n","\n","# -------------------\n","# Logistic loss\n","# -------------------\n","class LogisticLoss(nn.Module):\n","    def forward(self, preds, labels):\n","        return torch.mean((-labels * preds))\n","\n","# -------------------\n","# Dataset preparation\n","# -------------------\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","train_dataset = torchvision.datasets.MNIST(\n","    root=\"./data\",       # folder to store dataset\n","    train=True,          # True = training set\n","    download=True,       # download if not already\n","    transform=transform  # preprocessing\n",")\n","\n","def get_binary_task(dataset, class_a, class_b, num_samples):\n","    idx = np.where((np.array(dataset.targets) == class_a) |\n","                   (np.array(dataset.targets) == class_b))[0]\n","    idx = np.random.choice(idx, size=num_samples, replace=False)\n","    subset = Subset(dataset, idx)\n","    labels = dataset.targets[idx].clone()\n","    labels[labels == class_a] = 1\n","    labels[labels == class_b] = -1\n","    return subset, labels.to(device)\n","\n","# -------------------\n","# Model: 1 hidden layer MLP\n","# -------------------\n","class MLP(nn.Module):\n","    def __init__(self, input_dim=28*28, hidden_dim=256):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.act = nn.GELU()\n","        self.fc2 = nn.Linear(hidden_dim, 1, bias=False)\n","\n","        #self.fc2.weight.data = torch.from_numpy(np.random.choice([-1/hidden_dim**0.5, 1/hidden_dim**0.5], size=(1, hidden_dim))).float() # second layer weights, fixed\n","        #torch.nn.init.normal_(self.fc1.weight,\n","                #        mean=0, std=1)\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)\n","        x = self.act(self.fc1(x))\n","        return self.fc2(x).squeeze(-1)\n","\n","# -------------------\n","# Run multiple experiments\n","# -------------------\n","all_first_task_losses = []\n","all_per_task_losses = []\n","\n","for exp in range(num_experiments):\n","    print(f\"\\nExperiment {exp+1}/{num_experiments}\")\n","\n","    # Create new model for each experiment\n","    model = MLP(hidden_dim=hidden_dim).to(device)\n","    optimizer = optim.SGD(model.parameters(), lr=lr)\n","    criterion = HingeLoss()\n","\n","    # Select task data\n","    # Task 1: Dress (3) vs Ankle boot (9)\n","    task1_data, task1_labels = get_binary_task(train_dataset, 4, 5, num_samples_task1)\n","    X1_full = torch.stack([task1_data[i][0] for i in range(len(task1_data))]).to(device)\n","    y1_full = task1_labels\n","\n","    # Task 2: T-shirt/top (0) vs Coat (4)\n","    task2_data, task2_labels = get_binary_task(train_dataset, 6, 7, num_samples_task2)\n","    X2_full = torch.stack([task2_data[i][0] for i in range(len(task2_data))]).to(device)\n","    y2_full = task2_labels\n","\n","    tasks = [(X1_full, y1_full), (X2_full, y2_full)]\n","\n","    first_task_losses = []\n","    per_task_losses = [[], []]\n","\n","    # Sequential training\n","    for task_id, (X_full, y_full) in enumerate(tasks):\n","        for t in range(T):\n","            optimizer.zero_grad()\n","            logits = model(X_full)\n","            loss = criterion(logits, y_full)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Record losses\n","            with torch.no_grad():\n","                logits_first = model(X1_full)\n","                loss_first = criterion(logits_first, y1_full)\n","                first_task_losses.append(loss_first.item())\n","\n","                loss_current = criterion(logits, y_full)\n","                per_task_losses[task_id].append(loss_current.item())\n","\n","    all_first_task_losses.append(first_task_losses)\n","    all_per_task_losses.append(per_task_losses)\n","\n","# -------------------\n","# Compute average training loss over experiments\n","# -------------------\n","avg_first_task_loss = np.mean(np.array(all_first_task_losses), axis=0)\n","avg_per_task_losses = [\n","    np.mean(np.array([per_task_losses[task_id] for per_task_losses in all_per_task_losses]), axis=0)\n","    for task_id in range(2)\n","]\n","fig4 = plt.figure(\"Figure 1\")\n","fig4, ax = plt.subplots()\n","# Example: plot first task average loss\n","plt.plot(avg_first_task_loss, label=\"First task loss (avg)\")\n","#plt.plot(avg_per_task_losses[0], label=\"Task 1 loss (avg)\")\n","#plt.plot(avg_per_task_losses[1], label=\"Task 2 loss (avg)\")\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"Training loss\")\n","ax.set_xlim(left=0)\n","ax.set_ylim(top=0.5)\n","plt.legend()\n","plt.show()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"id":"R3j69meQ-WzL","executionInfo":{"status":"error","timestamp":1759806380243,"user_tz":420,"elapsed":3993,"user":{"displayName":"Hossein Taheri","userId":"10565663845480396106"}},"outputId":"81ee7800-5dc7-4f09-9837-460164ce231e"},"execution_count":1,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-174235370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}